# SkySense++

This repository is the official implementation of the paper "SkySense++: A Semantic-Enhanced Multi-Modal Remote Sensing Foundation Model Beyond SkySense for Earth Observation".

## üì¢ Latest Updates

üî•üî•üî• Last Updated on 2025.09.15 üî•üî•üî• 
- [2025.09.15] Add a [üåç project page](https://zqcrafts.github.io/SkySense-O/project.html) and fix some bugs.
- [2025.08.04] Our work has been published in [*Nature Machine Intelligence*](https://www.nature.com/articles/s42256-025-01078-8).
- [2025.03.23] Code for preprocessing/pretraining/application and [model weights](https://www.yuque.com/thinson/research/vpisiuswzbnriwvb?singleDoc=&language=en-us)(access code:bkl3) for models has been uploaded.
- [2025.03.14] updated optical images of JL-16 dataset in [Huggingface](https://huggingface.co/datasets/KKKKKKang/JL-16).
- [2025.03.12] updated sentinel-1 images and labels of JL-16 dataset  in [Zenodo](https://zenodo.org/records/15010418).
- [2025.03.09] created repo in [Zenodo](https://zenodo.org/records/15010418), datasets are uploading.
- [2024.11.13] updated details of pretrain and evaluation data.

## Pretrain Data

### RS-Semantic Dataset

We conduct semantic-enhanced pretraining on the RS-Semantic dataset, which consists of 13 datasets with pixel-level annotations. Below are the specifics of these datasets. (also see in [Zenodo](https://zenodo.org/records/15010418)).

| Dataset             | Modalities       | GSD(m) | Size                  | Categories | Download Link                                                                                 |
| ------------------- | ---------------- | ------ | --------------------- | ---------- | --------------------------------------------------------------------------------------------- |
| Five Billion Pixels | Gaofen-2         | 4      | 6800x7200             | 24         | [Download](https://x-ytong.github.io/project/Five-Billion-Pixels.html)                           |
| Potsdam             | Airborne         | 0.05   | 6000x6000             | 5          | [Download](https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-potsdam.aspx)     |
| Vaihingen           | Airborne         | 0.05   | 2494x2064             | 5          | [Download](https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-vaihingen.aspx)   |
| Deepglobe           | WorldView        | 0.5    | 2448x2448             | 6          | [Download](https://www.kaggle.com/datasets/balraj98/deepglobe-land-cover-classification-dataset) |
| iSAID               | Multiple Sensors | -      | 800x800 to 4000x13000 | 15         | [Download](https://captain-whu.github.io/iSAID/index.html)                                       |
| LoveDA              | Spaceborne       | 0.3    | 1024x1024             | 7          | [Download](https://github.com/Junjue-Wang/LoveDA)                                                |
| DynamicEarthNet     | WorldView        | 0.3    | 1024x1024             | 7          | [Download](https://github.com/aysim/dynnet)                                                      |
|                     | Sentinel-2*      | 10     | 32x32                 |            |                                                                                               |
|                     | Sentinel-1*      | 10     | 32x33                 |            |                                                                                               |
| Pastis-MM           | WorldView        | 0.3    | 1024x1024             | 18         | [Download](https://github.com/VSainteuf/pastis-benchmark)                                        |
|                     | Sentinel-2*      | 10     | 32x32                 |            |                                                                                               |
|                     | Sentinel-1*      | 10     | 32x33                 |            |                                                                                               |
| C2Seg-AB            | Sentinel-2*      | 10     | 128x128               | 13         | [Download](https://github.com/danfenghong/RSE_Cross-city)                                        |
|                     | Sentinel-1*      | 10     | 128x128               |            |                                                                                               |
| FLAIR               | Spot-5           | 0.2    | 512x512               | 12         | [Download](https://github.com/IGNF/FLAIR-2)                                                      |
|                     | Sentinel-2*      | 10     | 40x40                 |            |                                                                                               |
| DFC20               | Sentinel-2       | 10     | 256x256               | 9          | [Download](https://ieee-dataport.org/competitions/2020-ieee-grss-data-fusion-contest)            |
|                     | Sentinel-1       | 10     | 256x256               |            |                                                                                               |
| S2-naip             | NAIP             | 1      | 512x512               | 32         | [Download](https://huggingface.co/datasets/allenai/s2-naip)                                      |
|                     | Sentinel-2*      | 10     | 64x64                 |            |                                                                                               |
|                     | Sentinel-1*      | 10     | 64x64                 |            |                                                                                               |
| JL-16               | Jilin-1          | 0.72   | 512x512               | 16         | [Download](https://zenodo.org/records/15010418)                                                  |
|                     | Sentinel-1*      | 10     | 40x40                 |            |                                                                                               |

*\* for time-series data.*

### RS-Representation Dataset

The pretraining list is in the [Zenodo](https://zenodo.org/records/15068572)- `rep_data_list.tar`. The download and process scripts are in [tools/pretraining_data_builder](tools/pretraining_data_builder).

## EO Benchmark

We evaluate our SkySense++ on 12 typical Earth Observation (EO) tasks across 7 domains: *agriculture*, *forestry*, *oceanography*, *atmosphere*, *biology*, *land surveying*, and *disaster management*. The detailed information about the datasets used for evaluation is as follows.

| Domain              | Task type                   | Dataset               | Modalities             | GSD  | Image size             | Download Link                                                                                                                                 | Notes |
| ------------------- | --------------------------- | --------------------- | ---------------------- | ---- | ---------------------- | --------------------------------------------------------------------------------------------------------------------------------------------- | ----- |
| Agriculture         | Crop classification         | Germany               | Sentinel-2*            | 10   | 24x24                  | [Download](https://github.com/michaeltrs/DeepSatModels/tree/main/data)                                                                           |       |
| Foresetry           | Tree species classification | TreeSatAI-Time-Series | Airborne,              | 0.2  | 304x304                | [Download](http://example.com/download/treesatai-time-series)                                                                                    |       |
|                     |                             |                       | Sentinel-2*            | 10   | 6x6                    |                                                                                                                                               |       |
|                     |                             |                       | Sentinel-1*            | 10   | 6x6                    |                                                                                                                                               |       |
|                     | Deforestation segmentation  | Atlantic              | Sentinel-2             | 10   | 512x512                | [Download](https://github.com/davej23/attention-mechanism-unet)                                                                                  |       |
| Oceanography        | Oil spill segmentation      | SOS                   | Sentinel-1             | 10   | 256x256                | [Download](https://grzy.cug.edu.cn/zhuqiqi/en/yjgk/32384/list/index.htm)                                                                         |       |
| Atmosphere          | Air pollution regression    | 3pollution            | Sentinel-2             | 10   | 200x200                | [Download](https://github.com/CoDIS-Lab/AQNet)                                                                                                   |       |
|                     |                             |                       | Sentinel-5P            | 2600 | 120x120                |                                                                                                                                               |       |
| Biology             | Wildlife detection          | Kenya                 | Airborne               | -    | 3068x4603              | [Download](https://data.4tu.nl/articles/_/12713903/1)                                                                                            |       |
| Land surveying      | LULC mapping                | C2Seg-BW              | Gaofen-6               | 10   | 256x256                | [Download](https://github.com/danfenghong/RSE_Cross-city)                                                                                        |       |
|                     |                             |                       | Gaofen-3               | 10   | 256x256                |                                                                                                                                               |       |
|                     | Change detection            | dsifn-cd              | GoogleEarth            | 0.3  | 512x512                | [Download](https://github.com/GeoZcx/A-deeply-supervised-image-fusion-network-for-change-detection-in-remote-sensing-images/tree/master/dataset) |       |
| Disaster management | Flood monitoring            | Flood-3i              | Airborne               | 0.05 | 256 √ó 256             | [Download](https://drive.google.com/drive/folders/1FMAKf2sszoFKjq0UrUmSLnJDbwQSpfxR)                                                             |       |
|                     |                             | C2SMSFloods           | Sentinel-2, Sentinel-1 | 10   | 512x512                | [Download](https://beta.source.coop/c2sms/)                                                                                                      |       |
|                     | Wildfire monitoring         | CABUAR                | Sentinel-2             | 10   | 5490 √ó 5490           | [Download](https://github.com/DarthReca/CaBuAr)                                                                                                  |       |
|                     | Landslide mapping           | GVLM                  | GoogleEarth            | 0.3  | 1748x1748 ~ 10808x7424 | [Download](https://github.com/zxk688/GVLM)                                                                                                       |       |
|                     | Building damage assessment  | xBD                   | WorldView              | 0.3  | 1024x1024              | [Download](https://xview2.org/)                                                                                                                  |       |

*\* for time-series data.*

## Implementation Code

### Structure

This project mainly contains the following parts.

```plain
./
‚îú‚îÄ‚îÄ antmmf/                             # antmmf framework code
‚îú‚îÄ‚îÄ configs/                   
‚îÇ   ‚îú‚îÄ‚îÄ eval_skysense_pp_flood3i.yml    # eval cfg on flood3i                
‚îÇ   ‚îî‚îÄ‚îÄ pretrain_skysensepp.yml         # pretrain cfg
‚îú‚îÄ‚îÄ finetune/                           # finetuning code
‚îÇ   ‚îú‚îÄ‚îÄ configs/                        # finetuning configs
‚îÇ   ‚îú‚îÄ‚îÄ mmseg/                          # mmseg library
‚îÇ   ‚îú‚îÄ‚îÄ requirements/                   # mmseg install requirements folder
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt                # mmseg install requirements
‚îÇ   ‚îú‚îÄ‚îÄ setup.py                        # mmseg setup file
‚îÇ   ‚îî‚îÄ‚îÄ tools/                          # mmseg utils
‚îú‚îÄ‚îÄ lib/                                # model implementation
‚îÇ   ‚îú‚îÄ‚îÄ datasets/                       # datasets for evaluation
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/                     # evaluation code
‚îÇ   ‚îú‚îÄ‚îÄ models/                         # model architecture
‚îÇ   ‚îú‚îÄ‚îÄ predictors/                     # inference code
‚îÇ   ‚îú‚îÄ‚îÄ task/                           # task code
‚îÇ   ‚îú‚îÄ‚îÄ trainer/                        # trainer code
‚îÇ   ‚îú‚îÄ‚îÄ utils/                          # library code
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py                     # packages init file
‚îú‚îÄ‚îÄ pretrain/                           # pretrain ckpts
‚îú‚îÄ‚îÄ tools/                              # tools ckpts
‚îÇ   ‚îú‚îÄ‚îÄ pretraining_data_builder        # pretraining dataset builder
‚îÇ   ‚îú‚îÄ‚îÄ run_1shot_flood3i.sh            # datasets for evaluation
‚îÇ   ‚îú‚îÄ‚îÄ run_ft_atlantic.sh              # run ft script
‚îÇ   ‚îú‚îÄ‚îÄ run_pretrain.sh                 # run pretrain script
‚îÇ   ‚îî‚îÄ‚îÄ run.py                          # Program entry point
‚îî‚îÄ‚îÄ readme.md                           # project readme
```

### Environment

Each machine for implementating the pretraining or fintuning are with *Alibaba Group Enterprise Linux(7.2)* and *Python 3.8.10*. The pretraining and finetuning code are implemented on severs with *Intel(R) Xeon(R) Platinum 8369B CPU @ 2.90GHz* and *Nvidia A100 GPUS*.

### Pretraining

To run our pretraining code, please install dependency packages. (Instalazation takes about 14 minutes on a node with Intel(R) Xeon(R) Platinum 8369B CPU @ 2.90GHz and 8 A100 GPUs.)

```plain
torch==1.13.1
atorch==0.1.3
torchvision==0.14.1
mmcv-full==1.7.1
mmsegmentation==0.30.0
mmcls==0.25.0
timm==0.6.13
gdal==3.4.0
scikit-image==0.19.3
```

Step1. Install the above packages and clone [antmmf framework](https://github.com/alipay/Ant-Multi-Modal-Framework):

```bash
git clone https://github.com/alipay/Ant-Multi-Modal-Framework.git antmmf/
```

Step2. Download the pretraining datasets in [Zenodo](https://zenodo.org/records/15010418) and orgnize them as follows:

```
pretrain_datasets
‚îú‚îÄ‚îÄ dynamic-mm                          # multi-modal dynamic-mm datasets
‚îÇ   ‚îú‚îÄ‚îÄ images_hr                       # hr images
‚îÇ   ‚îú‚îÄ‚îÄ images_s2                       # sentinel-2 images
‚îÇ   ‚îú‚îÄ‚îÄ images_s1                       # sentinel-1 images
‚îÇ   ‚îú‚îÄ‚îÄ labels                          # segmentation annotations
‚îÇ   ‚îú‚îÄ‚îÄ dynamic-mm_train.json           # train list file
‚îÇ   ‚îî‚îÄ‚îÄ dynamic-mm_val.json             # val list file
‚îú‚îÄ‚îÄ fbp                                 # single-modal fbp datasets
‚îÇ   ‚îú‚îÄ‚îÄ images                          # input gaofen-2 images
‚îÇ   ‚îú‚îÄ‚îÄ labels                          # segmentation annotations
‚îÇ   ‚îú‚îÄ‚îÄ fbp_train.json                  # train list file
‚îÇ   ‚îî‚îÄ‚îÄ fbp_val.json                    # val list file
‚îî‚îÄ‚îÄ ......                       
```

The `<dataset>_<train/val>.json` is used to read information for training and validation, with a unified organizational format:

```json
[
  {
    "hr_path": "dataset_name/images_hr/<img_name>.png", // hr info c,h,w
    "s2_path": ["dataset_name/images_s2/<img_name>_20240101.npz", "dataset_name/images_s2/<img_name>_20240103.npz"], // s2 c,h,w
    "s1_path": ["dataset_name/images_s1/<img_name>_20240104.npz", "dataset_name/images_s1/<img_name>_20240108.npz"], // s1 c,h,w
    "target_path": "dataset_name/labels/<img_name>.png", // annotation info
    "type": "dataset_name", // dataset_name
    "classes": [            // Included categories
            0,
            2,
            4,
            5
        ]
  },
  {
    ...
  }
]
```

Step3. Download the pretraining weights of SkySense [here](https://www.yuque.com/thinson/research/vpisiuswzbnriwvb?singleDoc=&language=en-us) (access code:bkl3) and move it to `pretrain/`

Step4. Run the pretrain code on 4 nodes (each node with 8 A100 gpus):

```
bash tools/run_pretrain.sh <node_rank:0-3> <master_ip_address>
```

For example, if the ip adress of master node is *192.168.112.10*, the command for node *1* is:

```
bash tools/run_pretrain.sh 1 192.168.112.10
```

### Downstream 1-shot application

#### Requirments

To run our code, please install dependency packages. ( Instalazation takes about 10 minutes on a sever with Intel(R) Xeon(R) Platinum 8369B CPU @ 2.90GHz and 2 A100 GPUs.)

```plain
torch==1.13.1
atorch==0.1.3
torchvision==0.14.1
mmcv-full==1.7.1
mmcls==0.25.0
mmsegmentation==0.30.0
timm==0.6.13
gdal==3.4.0
scikit-image==0.19.3
```

#### Run steps

step1. Clone [antmmf framework](https://github.com/alipay/Ant-Multi-Modal-Framework). and install the above packages:

```plain
git clone https://github.com/alipay/Ant-Multi-Modal-Framework.git antmmf/
```

step1. Download the flood-3i dataset at [here](https://drive.google.com/drive/folders/1FMAKf2sszoFKjq0UrUmSLnJDbwQSpfxR).

step2. Using the above pretraining wieights or download the pretrained model weights [here](https://www.yuque.com/thinson/research/vpisiuswzbnriwvb?singleDoc=&language=en-us) (access code:bkl3).

step3. Run the script for evaluating 1-shot performance on flood-3i:

```plain
bash tools/run_1shot.sh <gpu_idx> flood-3i(dataset_name)
```

### Downstream finetuning application

#### Requirments

We build our fine-tuning application code on the [openmmlab framework](https://github.com/open-mmlab/mmcv).

To run our code, please install dependency packages. ( Instalazation takes about 10 minutes on a sever with Intel(R) Xeon(R) Platinum 8369B CPU @ 2.90GHz and 2 A100 GPUs.)

```plain
torch==1.13.1
torchvision==0.14.1
mmcv-full==2.1.0
mmpretrain==1.2.0
mmsegmentation==1.2.2
mmdetection==3.3.0
timm==0.6.13
gdal==3.4.0
scikit-image==0.19.3
```

#### Run steps

Step1. Install the mmsegmentation framework under the instrction in [here](https://mmsegmentation.readthedocs.io/en/latest/index.html)

Step2. Download the evaluation datsets. We take  Atlantic dataset for deforestation segmentation as an example. Download the Atlantic dataset  at [here](https://github.com/davej23/attention-mechanism-unet).

Step3. Use your pretrained model weights or download the model weights: [here](https://www.yuque.com/thinson/research/vpisiuswzbnriwvb?singleDoc=&language=en-us) (access code:bkl3)

Step4. Run the finetuning script. We take the Atlantic dataset as an example:

```bash
bash tools/run_finetune.sh configs/atlantic.py
```

## Acknowledgments

This projects are mainly built on the following projects:

+ [antmmf](https://github.com/alipay/Ant-Multi-Modal-Framework)
+ [mmcv](https://github.com/open-mmlab/mmcv)
+ [mmpretrain](https://github.com/open-mmlab/mmpretrain)
+ [mmsegmentation](https://github.com/open-mmlab/mmsegmentation)
+ [mmdetection](https://github.com/open-mmlab/mmdetection)
+ [Painter](https://github.com/baaivision/Painter)

## License

The codes of this project is licensed under Apache 2.0(https://www.apache.org/licenses/LICENSE-2.0.html).

## Citation
If you find our repo useful, please consider giving a star and citation:

```
@article{wu2025semantic,
  author       = {Wu, Kang and Zhang, Yingying and Ru, Lixiang and Dang, Bo and Lao, Jiangwei and Yu, Lei and Luo, Junwei and Zhu, Zifan and Sun, Yue and Zhang, Jiahao and Zhu, Qi and Wang, Jian and Yang, Ming and Chen, Jingdong and Zhang, Yongjun and Li, Yansheng},
  title        = {A semantic‚Äëenhanced multi‚Äëmodal remote sensing foundation model for Earth observation},
  journal      = {Nature Machine Intelligence},
  year         = {2025},
  doi          = {10.1038/s42256-025-01078-8},
  url          = {https://doi.org/10.1038/s42256-025-01078-8}
}

@inproceedings{guo2024skysense,
    author    = {Guo, Xin and Lao, Jiangwei and Dang, Bo and Zhang, Yingying and Yu, Lei and Ru, Lixiang and Zhong, Liheng and Huang, Ziyuan and Wu, Kang and Hu, Dingxiang and He, Huimei and Wang, Jian and Chen, Jingdong and Yang, Ming and Zhang, Yongjun and Li, Yansheng},
    title     = {SkySense: A Multi-Modal Remote Sensing Foundation Model Towards Universal Interpretation for Earth Observation Imagery},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {27672-27683}
}

@inproceedings{zhu2025skysenseo,
  title={Skysense-o: Towards open-world remote sensing interpretation with vision-centric visual-language modeling},
  author={Zhu, Qi and Lao, Jiangwei and Ji, Deyi and Luo, Junwei and Wu, Kang and Zhang, Yingying and Ru, Lixiang and Wang, Jian and Chen, Jingdong and Yang, Ming and others},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={14733--14744},
  year={2025}
}

@article{luo2024skysensegpt,
  title={Skysensegpt: A fine-grained instruction tuning dataset and model for remote sensing vision-language understanding},
  author={Luo, Junwei and Pang, Zhen and Zhang, Yongjun and Wang, Tingzhu and Wang, Linlin and Dang, Bo and Lao, Jiangwei and Wang, Jian and Chen, Jingdong and Tan, Yihua and others},
  journal={arXiv preprint arXiv:2406.10100},
  year={2024}
}
```
## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=kang-wu/SkySensePlusPlus&type=Date)](https://www.star-history.com/#kang-wu/SkySensePlusPlus&Date)